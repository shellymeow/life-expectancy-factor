{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score, ShuffleSplit, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Immunization Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'HepB3.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# https://www.who.int/data/gho\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 'Hepatitis B (HepB3) immunization coverage among 1-year-olds (%)'\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHepB3.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m HepB3 \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpatialDimValueCode\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPeriod\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m'\u001b[39m]]\t\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 'Measles-containing-vaccine first-dose (MCV1) immunization coverage among 1-year-olds (%)'\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/life_expectancy/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/life_expectancy/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/life_expectancy/lib/python3.8/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/life_expectancy/lib/python3.8/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/life_expectancy/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/life_expectancy/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/life_expectancy/lib/python3.8/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'HepB3.csv'"
     ]
    }
   ],
   "source": [
    "# original data: <https://www.who.int/data/gho>\n",
    "\n",
    "data_path = \"\"\n",
    "\n",
    "# 'Hepatitis B (HepB3) immunization coverage among 1-year-olds (%)'\n",
    "data = pd.read_csv('HepB3.csv')\n",
    "HepB3 = data[['SpatialDimValueCode','Period','Value']]\t\n",
    "\n",
    "# 'Measles-containing-vaccine first-dose (MCV1) immunization coverage among 1-year-olds (%)'\n",
    "data = pd.read_csv('MCV1.csv')\n",
    "MCV1 = data[['SpatialDimValueCode','Period','Value']]\n",
    "\n",
    "# 'Measles-containing-vaccine second-dose (MCV2) immunization coverage by the nationally recommended age (%)'\n",
    "data = pd.read_csv('MCV2.csv')\n",
    "MCV2 = data[['SpatialDimValueCode','Period','Value']]\t\n",
    "\n",
    "# 'BCG immunization coverage among 1-year-olds (%)'\n",
    "data = pd.read_csv('BCG.csv')\n",
    "BCG = data[['SpatialDimValueCode','Period','Value']]\t\n",
    "\n",
    "# 'Polio (Pol3) immunization coverage among 1-year-olds (%)'\n",
    "data = pd.read_csv('Pol3.csv')\n",
    "Pol3 = data[['SpatialDimValueCode','Period','Value']]\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immu = pd.merge(MCV1, MCV2, how='outer', on=['SpatialDimValueCode','Period'])\n",
    "immu = immu.rename(columns={'Value_x': 'MCV1', 'Value_y': 'MCV2'})\n",
    "\n",
    "immu = pd.merge(HepB3, immu, how='outer', on=['SpatialDimValueCode','Period'])\n",
    "immu = immu.rename(columns={'Value': 'HepB3'})\n",
    "\n",
    "immu = pd.merge(BCG, immu, how='outer', on=['SpatialDimValueCode','Period'])\n",
    "immu = immu.rename(columns={'Value': 'BCG'})\n",
    "\n",
    "immu = pd.merge(Pol3, immu, how='outer', on=['SpatialDimValueCode','Period'])\n",
    "immu = immu.rename(columns={'Value': 'Pol3', 'SpatialDimValueCode': 'Country','Period':'Year'})\n",
    "\n",
    "immu = immu[['Country','Year','HepB3','MCV1','MCV2','BCG','Pol3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immu.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "HepB3_2021 = immu[immu['Year']==2021][['Country','HepB3']].dropna()\n",
    "HepB3_2021 = pd.merge(world, HepB3_2021, left_on='iso_a3',right_on='Country')\n",
    "HepB3_2021.plot(column='HepB3',figsize=(15,10),legend=True, legend_kwds={'shrink': 0.4})\n",
    "plt.title('Hepatitis B (HepB3) immunization coverage among 1-year-olds (%) in 2021',fontsize=15)\n",
    "\n",
    "plt.figure()\n",
    "MCV1_2021 = immu[immu['Year']==2021][['Country','MCV1']].dropna()\n",
    "MCV1_2021 = pd.merge(world, MCV1_2021, left_on='iso_a3',right_on='Country')\n",
    "MCV1_2021.plot(column='MCV1',figsize=(15,10),legend=True, legend_kwds={'shrink': 0.4})\n",
    "plt.title('Measles-containing-vaccine first-dose (MCV1) immunization coverage among 1-year-olds (%) in 2021',fontsize=15)\n",
    "\n",
    "plt.figure()\n",
    "MCV2_2021 = immu[immu['Year']==2021][['Country','MCV2']].dropna()\n",
    "MCV2_2021 = pd.merge(world, MCV2_2021, left_on='iso_a3',right_on='Country')\n",
    "MCV2_2021.plot(column='MCV2',figsize=(15,10),legend=True, legend_kwds={'shrink': 0.4})\n",
    "plt.title('Measles-containing-vaccine second-dose (MCV2) immunization coverage among 1-year-olds (%) in 2021',fontsize=15)\n",
    "\n",
    "plt.figure()\n",
    "BCG_2021 = immu[immu['Year']==2021][['Country','BCG']].dropna()\n",
    "BCG_2021 = pd.merge(world, BCG_2021, left_on='iso_a3',right_on='Country')\n",
    "BCG_2021.plot(column='BCG',figsize=(15,10),legend=True, legend_kwds={'shrink': 0.4})\n",
    "plt.title('BCG immunization coverage among 1-year-olds (%) in 2021',fontsize=15)\n",
    "\n",
    "plt.figure()\n",
    "Pol3_2021 = immu[immu['Year']==2021][['Country','Pol3']].dropna()\n",
    "Pol3_2021 = pd.merge(world, Pol3_2021, left_on='iso_a3',right_on='Country')\n",
    "Pol3_2021.plot(column='Pol3',figsize=(15,10),legend=True, legend_kwds={'shrink': 0.4})\n",
    "plt.title('Polio (Pol3) immunization coverage among 1-year-olds (%) in 2021',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Alcohol Consumption Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)'\n",
    "alc = pd.read_csv('alc.csv', dtype={'Value':object})\n",
    "# Filter alcohol consumption by alcohol type = total\n",
    "alc = alc[alc['Dim1ValueCode']=='SA_TOTAL'][['SpatialDimValueCode','Period','FactValueNumeric']]\n",
    "alc = alc.rename(columns={'SpatialDimValueCode': 'Country','Period':'Year','FactValueNumeric': 'AlcoholConsumption'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alc.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alc_2019 = alc[alc['Year']==2019][['Country','AlcoholConsumption']].dropna()\n",
    "alc_2019 = pd.merge(world, alc_2019, left_on='iso_a3',right_on='Country')\n",
    "alc_2019.plot(column='AlcoholConsumption',figsize=(15,10),legend=True, legend_kwds={'shrink': 0.4})\n",
    "plt.title('Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol) in 2019',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tobacco Prevalence Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Estimate of current tobacco smoking prevalence (%)'\n",
    "data = pd.read_csv('tob.csv')\n",
    "# Filter tobacco prevalence by sex = both sex \n",
    "tob = data[(data['IndicatorCode']=='M_Est_smk_curr_std') & (data['Dim1']== 'Both sexes')][['SpatialDimValueCode','Period','Value']]\n",
    "tob = tob.rename(columns={'SpatialDimValueCode': 'Country','Period':'Year','Value': 'TobaccoPrevalence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tob.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tob_2020 = tob[tob['Year']==2020][['Country','TobaccoPrevalence']].dropna()\n",
    "tob_2020 = pd.merge(world, tob_2020, left_on='iso_a3',right_on='Country')\n",
    "tob_2020.plot(column='TobaccoPrevalence',figsize=(15,10),legend=True, legend_kwds={'shrink': 0.4})\n",
    "plt.title('Tobacco use prevalence (%) in 2020',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Health Expenditure Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current health expenditure (CHE) per capita in PPP\n",
    "data = pd.read_csv('CHE_PPP.csv')\n",
    "data.columns\n",
    "che_ppp = data[['SpatialDimValueCode','Period','FactValueNumeric']]\n",
    "che_ppp = che_ppp.rename(columns={'SpatialDimValueCode': 'Country','Period':'Year','FactValueNumeric': 'CHE_PPP'})\n",
    "\n",
    "# Current health expenditure (CHE) as percentage of gross domestic product (GDP) (%)\n",
    "data = pd.read_csv('CHE_GDP.csv')\n",
    "che_gdp = data[['SpatialDimValueCode','Period','FactValueNumeric']]\n",
    "che_gdp = che_gdp.rename(columns={'SpatialDimValueCode': 'Country','Period':'Year','FactValueNumeric': 'CHE_GDP'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "che = pd.merge(che_ppp, che_gdp, how='outer', on=['Country','Year'])\n",
    "che.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHE_PPP_2019 = che[che['Year']==2019][['Country','CHE_PPP']].dropna()\n",
    "CHE_PPP_2019 = pd.merge(world, CHE_PPP_2019, left_on='iso_a3',right_on='Country')\n",
    "CHE_PPP_2019.plot(column='CHE_PPP',figsize=(15,10),legend=True, legend_kwds={'shrink': 0.4})\n",
    "plt.title('Health expenditure (CHE) per capita in PPP in 2019',fontsize=15)\n",
    "\n",
    "plt.figure()\n",
    "CHE_GDP_2019 = che[che['Year']==2019][['Country','CHE_GDP']].dropna()\n",
    "CHE_GDP_2019 = pd.merge(world, CHE_GDP_2019, left_on='iso_a3',right_on='Country')\n",
    "CHE_GDP_2019.plot(column='CHE_GDP',figsize=(15,10),legend=True, legend_kwds={'shrink': 0.4})\n",
    "plt.title('Health expenditure (CHE) per capita in GDP in 2019',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Under-/Overweight/Obesity Prevalence ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevalence of underweight among adults, BMI < 18 (age-standardized estimate) (%)\n",
    "data = pd.read_csv('underweight.csv')\n",
    "# Filter underweight prevalence by sex = both sex\n",
    "underweight = data[data['Dim1']=='Both sexes'][['SpatialDimValueCode','Period','FactValueNumeric']]\n",
    "underweight = underweight.rename(columns={'SpatialDimValueCode': 'Country','Period':'Year','FactValueNumeric': 'UnderweightPrevalence'})\n",
    "\n",
    "# Prevalence of overweight among adults, BMI >= 25 (age-standardized estimate) (%)\n",
    "data = pd.read_csv('overweight.csv')\n",
    "# Filter overweight prevalence by sex = both sex\n",
    "overweight = data[data['Dim1']=='Both sexes'][['SpatialDimValueCode','Period','FactValueNumeric']]\n",
    "overweight = overweight.rename(columns={'SpatialDimValueCode': 'Country','Period':'Year','FactValueNumeric':'OverweightPrevalence'})\n",
    "\n",
    "# Prevalence of obesity among adults, BMI >= 30 (age-standardized estimate) (%)\n",
    "data = pd.read_csv('obesity.csv')\n",
    "# Filter overweight prevalence by sex = both sex\n",
    "obesity = data[data['Dim1']=='Both sexes'][['SpatialDimValueCode','Period','FactValueNumeric']]\n",
    "obesity = obesity.rename(columns={'SpatialDimValueCode': 'Country','Period':'Year','FactValueNumeric': 'ObesityPrevalence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abn_wgt = pd.merge(underweight, overweight, how='outer', on=['Year','Country'])\n",
    "abn_wgt = pd.merge(obesity, abn_wgt, how='outer', on=['Year','Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abn_wgt.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "underweight_2016 = underweight[underweight['Year']==2016][['Country','UnderweightPrevalence']].dropna()\n",
    "underweight_2016 = pd.merge(world, underweight_2016, left_on='iso_a3',right_on='Country')\n",
    "underweight_2016.plot(column='UnderweightPrevalence',figsize=(15,10),legend=True, legend_kwds={'shrink': 0.4})\n",
    "plt.title('Prevalence of underweight among adults, BMI < 18 (age-standardized estimate) (%) in 2016',fontsize=15)\n",
    "\n",
    "plt.figure()\n",
    "overweight_2016 = overweight[overweight['Year']==2016][['Country','OverweightPrevalence']].dropna()\n",
    "overweight_2016 = pd.merge(world, overweight_2016, left_on='iso_a3',right_on='Country')\n",
    "overweight_2016.plot(column='OverweightPrevalence',figsize=(15,10),legend=True, legend_kwds={'shrink': 0.4})\n",
    "plt.title('Prevalence of overweight among adults, BMI >= 25 (age-standardized estimate) (%) in 2016',fontsize=15)\n",
    "\n",
    "plt.figure()\n",
    "obesity_2016 = obesity[obesity['Year']==2016][['Country','ObesityPrevalence']].dropna()\n",
    "obesity_2016 = pd.merge(world, obesity_2016, left_on='iso_a3',right_on='Country')\n",
    "obesity_2016.plot(column='ObesityPrevalence',figsize=(15,10),legend=True, legend_kwds={'shrink': 0.4})\n",
    "plt.title('Prevalence of obesity among adults, BMI >= 30 (age-standardized estimate) (%) in 2016',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Life Expectancy Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Life expectancy at birth (years)'\n",
    "data = pd.read_csv('life_exp.csv')\n",
    "# Filter life expectancy by sex = both sex\n",
    "life_exp = data[(data['Dim1']=='Both sexes') & (data['IndicatorCode']== 'WHOSIS_000001')][['SpatialDimValueCode','Period','Value']]\n",
    "life_exp = life_exp.rename(columns={'SpatialDimValueCode': 'Country','Period':'Year','Value': 'LifeExpectancy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_exp.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_exp[life_exp['Year']==2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_exp_2019 = life_exp[life_exp['Year']==2019][['Country','LifeExpectancy']].dropna()\n",
    "life_exp_2019 = pd.merge(world, life_exp_2019, left_on='iso_a3',right_on='Country')\n",
    "life_exp_2019.plot(column='LifeExpectancy',figsize=(15,10),legend=True, legend_kwds={'shrink': 0.4})\n",
    "plt.title('Life expectancy at birth (years) in 2019',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(immu, alc, how='outer', on=['Country','Year'])\n",
    "data = data.merge(tob, how='outer', on=['Country','Year'])\n",
    "data = data.merge(che, how='outer', on=['Country','Year'])\n",
    "data = data.merge(abn_wgt, how='outer', on=['Country','Year'])\n",
    "data = data.merge(life_exp, how='outer', on=['Country','Year'])\n",
    "# Drop NaN values\n",
    "data_dropna = data.dropna()\n",
    "data = data[(data['Year']>=2000) & (data['Year']<=2022)].groupby(['Country']).transform(lambda x: x.fillna(x.mean())).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicates\n",
    "num_dup = np.sum(data.duplicated())\n",
    "print(num_dup)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicates\n",
    "num_dup = np.sum(data_dropna.duplicated())\n",
    "print(num_dup)\n",
    "\n",
    "data_dropna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.boxplot(column=['HepB3','MCV1','MCV2','BCG','Pol3'])  \n",
    "plt.title('Immunization coverage')\n",
    "plt.figure()\n",
    "data.boxplot(column=['ObesityPrevalence','UnderweightPrevalence','OverweightPrevalence'])  \n",
    "plt.title('Abnormal weight prevalence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves regressor performance (Baseline+univariante+multivariante regressor, training/test mse, data dropNaN/fill average)\n",
    "mse_reg = np.zeros((14, 2, 2))\n",
    "mse_reg_std = np.zeros((14, 2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean value as baseline predictor ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data that fills the NaN value with average value\n",
    "X = data.drop(['LifeExpectancy','Year'], axis=1)\n",
    "features = X.columns\n",
    "y = data['LifeExpectancy'].to_numpy()\n",
    "X = scale(X)\n",
    "\n",
    "# Do multiple train-test split against noisy split to measure the performance\n",
    "splits = ShuffleSplit(n_splits=100, test_size=0.2, train_size=0.8, random_state=42)\n",
    "mse_train = np.zeros(splits.get_n_splits())\n",
    "mse_test = np.zeros(splits.get_n_splits())\n",
    "\n",
    "# Apply univariante linear regression on each train-test split\n",
    "for i, (train_index, test_index) in enumerate(splits.split(X, y)):\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    # Use dummy mean value as prediction\n",
    "    y_pred_train = y_train.mean() * np.ones_like(y_train) \n",
    "    y_pred_test = y_train.mean() * np.ones_like(y_test)\n",
    "    \n",
    "    # Compute the training and test MSE \n",
    "    mse_train[i] = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test[i] = mean_squared_error(y_test, y_pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train_std = mse_train.std()\n",
    "mse_test_std = mse_test.std()\n",
    "mse_reg_std[13,0,0] = mse_train_std\n",
    "mse_reg_std[13,1,0] = mse_test_std\n",
    "print(f'training_mse: {mse_train_std}, test_mse: {mse_test_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = mse_train.mean()\n",
    "mse_test = mse_test.mean()\n",
    "mse_reg[13,0,0] = mse_train\n",
    "mse_reg[13,1,0] = mse_test\n",
    "print(f'training_mse: {mse_train}, test_mse: {mse_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data that simply drops the NaN value\n",
    "X = data_dropna.drop(['LifeExpectancy','Year','Country'], axis=1)\n",
    "features = X.columns\n",
    "y = data_dropna['LifeExpectancy'].to_numpy()\n",
    "X = scale(X)\n",
    "\n",
    "# Do multiple train-test split against noisy split to measure the performance\n",
    "splits = ShuffleSplit(n_splits=100, test_size=0.2, train_size=0.8, random_state=42)\n",
    "mse_train = np.zeros(splits.get_n_splits())\n",
    "mse_test = np.zeros(splits.get_n_splits())\n",
    "\n",
    "# Apply univariante linear regression on each train-test split\n",
    "for i, (train_index, test_index) in enumerate(splits.split(X, y)):\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    # Use dummy mean value as prediction\n",
    "    y_pred_train = y_train.mean() * np.ones_like(y_train) \n",
    "    y_pred_test = y_train.mean() * np.ones_like(y_test)\n",
    "    \n",
    "    # Compute the training and test MSE \n",
    "    mse_train[i] = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test[i] = mean_squared_error(y_test, y_pred_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train_std = mse_train.std()\n",
    "mse_test_std = mse_test.std()\n",
    "mse_reg_std[13,0,1] = mse_train_std\n",
    "mse_reg_std[13,1,1] = mse_test_std\n",
    "print(f'mse_train_std: {mse_train_std}, mse_test_std: {mse_test_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = mse_train.mean()\n",
    "mse_test = mse_test.mean()\n",
    "mse_reg[13,0,1] = mse_train\n",
    "mse_reg[13,1,1] = mse_test\n",
    "print(f'training_mse: {mse_train}, test_mse: {mse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Linear Regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data that fills the NaN value with average value\n",
    "X = data.drop(['LifeExpectancy', 'Year'], axis=1)\n",
    "features = X.columns\n",
    "y = data['LifeExpectancy'].to_numpy()\n",
    "X = scale(X)\n",
    "\n",
    "# Split the dataset into training (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    plt.figure()\n",
    "    plt.scatter(X_train[:,i],y_train,marker='.',c='b',label='Training Set')\n",
    "    plt.scatter(X_test[:,i], y_test,marker='.',c='g',label='Test Set')\n",
    "    plt.title(f'Relationship between {features[i]} and Life Expectancy')\n",
    "    plt.xlabel(features[i])\n",
    "    plt.ylabel('Life Expectancy')\n",
    "    \n",
    "    # Train Univariate Linear Regression model on the training data \n",
    "    reg = LinearRegression()\n",
    "    reg.fit(np.expand_dims(X_train[:,i],1), y_train)\n",
    "    print(f'{features[i]}: reg_coef: {reg.coef_}, reg.intercept_: {reg.intercept_}')\n",
    "    \n",
    "    # Plot the regression line\n",
    "    x_range = np.linspace(np.min(X[:,i]),np.max(X[:,i]),100)\n",
    "    y_range = reg.predict(np.expand_dims(x_range,1))\n",
    "    plt.plot(x_range, y_range, c='r',label='Regression')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do multiple train-test split against noisy split to measure the performance\n",
    "splits = ShuffleSplit(n_splits=10, test_size=0.2, train_size=0.8, random_state=42)\n",
    "mse = np.zeros((splits.get_n_splits(), X.shape[1], 2))\n",
    "\n",
    "# Apply univariante linear regression on each train-test split\n",
    "for i, (train_index, test_index) in enumerate(splits.split(X, y)):\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    # Apply univariante linear regression on each feature\n",
    "    for j in range(X.shape[1]):\n",
    "        reg = LinearRegression()\n",
    "        reg.fit(np.expand_dims(X_train[:, j], 1), y_train)\n",
    "        \n",
    "        # Compute the training MSE\n",
    "        mse[i,j,0] = mean_squared_error(y_train, reg.predict(np.expand_dims(X_train[:, j], 1)))\n",
    "        # Compute the test MSE\n",
    "        mse[i,j,1] = mean_squared_error(y_test, reg.predict(np.expand_dims(X_test[:, j], 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate std of mse\n",
    "mse_train_uni = np.squeeze(mse[:,:,0])\n",
    "mse_test_uni = np.squeeze(mse[:,:,1])\n",
    "mse_train_uni_std = np.std(mse_train_uni, axis = 0)\n",
    "mse_test_uni_std = np.std(mse_test_uni, axis = 0)\n",
    "mse_reg_std[:12,0,0] = mse_train_uni_std\n",
    "mse_reg_std[:12,1,0] = mse_test_uni_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the training and test MSE over different splits\n",
    "mse = mse.mean(axis=0)\n",
    "mse_reg[:12,:,0] = mse\n",
    "mse = pd.DataFrame(mse,index=features,columns=['training_mse','test_mse'])\n",
    "mse.plot(kind='bar', title = 'Training MSE and Test MSE',colormap='viridis')\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display four factors: CHE_PPP, MVC2, underweight, Tabacco in one plot\n",
    "fig, axs = plt.subplots(2,2)\n",
    "fig.suptitle('Relationship between predicting variables and Life Expectancy')\n",
    "features_idx = [2,6,7,10]\n",
    "\n",
    "for i,idx in enumerate(features_idx):\n",
    "    ax = plt.subplot(2,2,i+1)\n",
    "    ax.scatter(X_train[:,idx],y_train,marker='.',c='darkorange',s=8,label='Training Set')\n",
    "    ax.scatter(X_test[:,idx], y_test,marker='.',c='olivedrab',s=8,label='Test Set')\n",
    "    ax.set_xlabel(features[idx])\n",
    "    ax.set_ylabel('Life Expectancy')\n",
    "    ax.set_ylim([20, 100])\n",
    "    \n",
    "    # Plot the regression line\n",
    "    x_range = np.linspace(np.min(X[:,idx]),np.max(X[:,idx]),100)\n",
    "    y_range = reg.predict(np.expand_dims(x_range,1))\n",
    "    ax.plot(x_range, y_range, c='r',label='Regression')\n",
    "    if i == 2:\n",
    "        ax.legend(loc='lower right')\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                        bottom=0.1,\n",
    "                        right=0.9,\n",
    "                        top=0.9,\n",
    "                        wspace=0.4,\n",
    "                        hspace=0.4)\n",
    "    \n",
    "fig.suptitle('Filling with mean')\n",
    "fig.savefig('uni_scatter_fill.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data that simply drops NaN value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data that simply drops NaN value\n",
    "X = data_dropna.drop(['LifeExpectancy','Year','Country'], axis=1)\n",
    "features = X.columns\n",
    "y = data_dropna['LifeExpectancy'].to_numpy()\n",
    "X = scale(X)\n",
    "\n",
    "# Split the dataset into training (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    plt.figure()\n",
    "    plt.scatter(X_train[:,i],y_train,marker='.',c='b',label='Training Set')\n",
    "    plt.scatter(X_test[:,i], y_test,marker='.',c='g',label='Test Set')\n",
    "    plt.title(f'Relationship between {features[i]} and Life Expectancy')\n",
    "    plt.xlabel(features[i])\n",
    "    plt.ylabel('Life Expectancy')\n",
    "    \n",
    "    # Train Univariate Linear Regression model on the training data \n",
    "    reg = LinearRegression()\n",
    "    reg.fit(np.expand_dims(X_train[:,i],1), y_train)\n",
    "    print(f'{features[i]}: reg_coef: {reg.coef_}, reg.intercept_: {reg.intercept_}')\n",
    "    \n",
    "    # Plot the regression line\n",
    "    x_range = np.linspace(np.min(X[:,i]),np.max(X[:,i]),100)\n",
    "    y_range = reg.predict(np.expand_dims(x_range,1))\n",
    "    plt.plot(x_range, y_range, c='r',label='Regression')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display four factors: CHE_PPP, MVC2, underweight, Tabacco in one plot\n",
    "fig, axs = plt.subplots(2,2)\n",
    "fig.suptitle('Relationship between predicting variables and Life Expectancy')\n",
    "features_idx = [2,6,7,10]\n",
    "\n",
    "for i,idx in enumerate(features_idx):\n",
    "    ax = plt.subplot(2,2,i+1)\n",
    "    ax.scatter(X_train[:,idx],y_train,marker='.',c='darkorange',s=8,label='Training Set')\n",
    "    ax.scatter(X_test[:,idx], y_test,marker='.',c='olivedrab',s=8,label='Test Set')\n",
    "    ax.set_xlabel(features[idx])\n",
    "    ax.set_ylabel('Life Expectancy')\n",
    "    ax.set_ylim([20, 100])\n",
    "    \n",
    "    # Plot the regression line\n",
    "    x_range = np.linspace(np.min(X[:,idx]),np.max(X[:,idx]),100)\n",
    "    y_range = reg.predict(np.expand_dims(x_range,1))\n",
    "    ax.plot(x_range, y_range, c='r',label='Regression')\n",
    "    if i == 2:\n",
    "        ax.legend(loc='lower right')\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                        bottom=0.1,\n",
    "                        right=0.9,\n",
    "                        top=0.9,\n",
    "                        wspace=0.4,\n",
    "                        hspace=0.4)\n",
    "    \n",
    "    \n",
    "fig.suptitle('Drop NaN')\n",
    "fig.savefig('uni_scatter_dropna.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do multiple train-test split against noisy split to measure the performance\n",
    "splits = ShuffleSplit(n_splits=100, test_size=0.2, train_size=0.8, random_state=42)\n",
    "mse = np.zeros((splits.get_n_splits(), X.shape[1], 2))\n",
    "\n",
    "# Apply univariante linear regression on each train-test split\n",
    "for i, (train_index, test_index) in enumerate(splits.split(X, y)):\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    # Apply univariante linear regression on each feature\n",
    "    for j in range(X.shape[1]):\n",
    "        reg = LinearRegression()\n",
    "        reg.fit(np.expand_dims(X_train[:, j], 1), y_train)\n",
    "        \n",
    "        # Compute the training MSE\n",
    "        mse[i,j,0] = mean_squared_error(y_train, reg.predict(np.expand_dims(X_train[:, j], 1)))\n",
    "        # Compute the test MSE\n",
    "        mse[i,j,1] = mean_squared_error(y_test, reg.predict(np.expand_dims(X_test[:, j], 1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate std of mse\n",
    "mse_train_uni = np.squeeze(mse[:,:,0])\n",
    "mse_test_uni = np.squeeze(mse[:,:,1])\n",
    "mse_train_uni_std = np.std(mse_train_uni, axis = 0)\n",
    "mse_test_uni_std = np.std(mse_test_uni, axis = 0)\n",
    "mse_reg_std[:12,0,1] = mse_train_uni_std\n",
    "mse_reg_std[:12,1,1] = mse_test_uni_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the training and test MSE over different splits\n",
    "mse = mse.mean(axis=0)\n",
    "mse_reg[:12,:,1] = mse\n",
    "mse = pd.DataFrame(mse,index=features,columns=['training_mse','test_mse'])\n",
    "mse.plot(kind='bar', title = 'Training MSE and Test MSE',colormap='viridis')\n",
    "\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression with LASSO Regularization ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data that fills the NaN value with average value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data that fills the NaN value with average value\n",
    "X = data.drop(['LifeExpectancy','Year'], axis=1)\n",
    "features = X.columns\n",
    "y = data['LifeExpectancy'].to_numpy()\n",
    "X = scale(X)\n",
    "\n",
    "# Split the dataset into training (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Multivariate Linear Regression model on the training data\n",
    "# Use LassoCV to find the best alpha value\n",
    "reg = LassoCV(cv=5, random_state=42)\n",
    "reg.fit(X_train, y_train)\n",
    "weights = {}\n",
    "for i in range(len(features)):\n",
    "    weights[features[i]] = reg.coef_[i]\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do multiple train-test split against noisy split to measure the performance\n",
    "splits = ShuffleSplit(n_splits=100, test_size=0.2, train_size=0.8, random_state=42)\n",
    "mse_train = np.zeros(splits.get_n_splits())\n",
    "mse_test = np.zeros(splits.get_n_splits())\n",
    "\n",
    "# Apply univariante linear regression on each train-test split\n",
    "for i, (train_index, test_index) in enumerate(splits.split(X, y)):\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    # Train Multivariate Linear Regression model on the training data\n",
    "    # Use Cross validation to find the best alpha value\n",
    "    reg = LassoCV(cv=5, random_state=42)\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute the training and test MSE \n",
    "    mse_train[i] = mean_squared_error(y_train, reg.predict(X_train))\n",
    "    mse_test[i] = mean_squared_error(y_test, reg.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train_mul_std = np.std(mse_train)\n",
    "mse_test_mul_std = np.std(mse_test)\n",
    "mse_reg_std[12,0,0]= mse_train_mul_std\n",
    "mse_reg_std[12,1,0] = mse_test_mul_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = mse_train.mean()\n",
    "mse_test = mse_test.mean()\n",
    "mse_reg[12,0,0] = mse_train\n",
    "mse_reg[12,1,0] = mse_train\n",
    "print(f'training_mse: {mse_train}, test_mse: {mse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data that simply drops NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data that simply drops NaN value\n",
    "X = data_dropna.drop(['LifeExpectancy','Year','Country'], axis=1)\n",
    "features = X.columns\n",
    "y = data_dropna['LifeExpectancy'].to_numpy()\n",
    "X = scale(X)\n",
    "\n",
    "# Split the dataset into training (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Multivariate Linear Regression model on the training data\n",
    "# Use LassoCV to find the best alpha value\n",
    "reg = LassoCV(cv=5, random_state=42)\n",
    "reg.fit(X_train, y_train)\n",
    "weights = {}\n",
    "for i in range(len(features)):\n",
    "    weights[features[i]] = reg.coef_[i]\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do multiple train-test split against noisy split to measure the performance\n",
    "splits = ShuffleSplit(n_splits=100, test_size=0.2, train_size=0.8, random_state=42)\n",
    "mse_train = np.zeros(splits.get_n_splits())\n",
    "mse_test = np.zeros(splits.get_n_splits())\n",
    "\n",
    "# Apply univariante linear regression on each train-test split\n",
    "for i, (train_index, test_index) in enumerate(splits.split(X, y)):\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    # Train Multivariate Linear Regression model on the training data\n",
    "    # Use Cross validation to find the best alpha value\n",
    "    reg = LassoCV(cv=5, random_state=42)\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute the training and test MSE \n",
    "    mse_train[i] = mean_squared_error(y_train, reg.predict(X_train))\n",
    "    mse_test[i] = mean_squared_error(y_test, reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train_mul_std = np.std(mse_train)\n",
    "mse_test_mul_std = np.std(mse_test)\n",
    "mse_reg_std[12,0,1]= mse_train_mul_std\n",
    "mse_reg_std[12,1,1] = mse_test_mul_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = mse_train.mean()\n",
    "mse_test = mse_test.mean()\n",
    "mse_reg[12,0,1] = mse_train\n",
    "mse_reg[12,1,1] = mse_test\n",
    "print(f'training_mse: {mse_train}, test_mse: {mse_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare mse between data groups\n",
    "mse_std_dropna_vs_fillmean = mse_reg_std[:,:,1]/ mse_reg_std[:,:,0]\n",
    "mse_std_dropna_vs_fillmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with mean compare mse_std between trainig and test\n",
    "mse_std_fillmean_train_test = mse_reg_std[:,1,0]/ mse_reg_std[:,0,0]\n",
    "mse_std_fillmean_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN compare mse_std between trainig and test\n",
    "mse_std_dropna_train_test = mse_reg_std[:,1,1]/ mse_reg_std[:,0,1]\n",
    "mse_std_dropna_train_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Feature Selection ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data that fills the NaN value with average value\n",
    "X = data.drop(['LifeExpectancy','Year'], axis=1)\n",
    "features = X.columns\n",
    "y = data['LifeExpectancy'].to_numpy()\n",
    "X = scale(X)\n",
    "\n",
    "# Train Multivariate Linear Regression model on the training data\n",
    "reg = LinearRegression()\n",
    "# Find the index of most important features using sequential feature selector\n",
    "for i in range(1, X.shape[1]):\n",
    "    sfs = SequentialFeatureSelector(reg, n_features_to_select=i, scoring ='neg_mean_squared_error',cv=5)\n",
    "    sfs.fit(X_train, y_train)\n",
    "    idx = sfs.get_support(indices=True)\n",
    "    print(f'The most important {i} features are: {features[idx]}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data that simply drops the NaN value\n",
    "X = data_dropna.drop(['LifeExpectancy','Year','Country'], axis=1)\n",
    "features = X.columns\n",
    "y = data_dropna['LifeExpectancy'].to_numpy()\n",
    "X = scale(X)\n",
    "\n",
    "# Train Multivariate Linear Regression model on the training data\n",
    "reg = LinearRegression()\n",
    "# Find the index of most important features using sequential feature selector\n",
    "for i in range(1, X.shape[1]):\n",
    "    sfs = SequentialFeatureSelector(reg, n_features_to_select=i, scoring ='neg_mean_squared_error',cv=5)\n",
    "    sfs.fit(X_train, y_train)\n",
    "    idx = sfs.get_support(indices=True)\n",
    "    print(f'The most important {i} features are: {features[idx]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data that fills the NaN value with average value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data that fills the NaN value with average value\n",
    "mse_hist = pd.DataFrame(mse_reg[:,:,0],\n",
    "                        index=features.append(pd.Index(['Multivariante with LASSO','Baseline with Mean'])),\n",
    "                        columns=['training_mse','test_mse'])\n",
    "mse_hist.plot(kind='bar',title = 'Training MSE and Test MSE',colormap='viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mse across groups with error bar\n",
    "labels = features.append(pd.Index(['Multivariante with LASSO','Baseline with Mean']))\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35 # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, mse_reg[:,0,0], width, color='darkorange',label='train',yerr=mse_reg_std[:,0,0], align='center', alpha=0.8, ecolor='black', capsize=1.5)\n",
    "rects2 = ax.bar(x + width/2, mse_reg[:,1,0], width, color='olivedrab',label='test',yerr=mse_reg_std[:,1,0], align='center', alpha=0.5, ecolor='black', capsize=1.5)\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('Filling with mean')\n",
    "ax.set_xticks(x, labels,rotation=90)\n",
    "# ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.set_ylim([0,70])\n",
    "fig.savefig('mse_fill.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data that simply drops the NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data that simply drops the NaN value\n",
    "mse_hist = pd.DataFrame(mse_reg[:,:,1],\n",
    "                        index=features.append(pd.Index(['Multivariante with LASSO','Baseline with Mean'])),\n",
    "                        columns=['training_mse','test_mse'])\n",
    "mse_hist.plot(kind='bar',title = 'Training MSE and Test MSE',colormap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mse across groups with error bar\n",
    "labels = features.append(pd.Index(['Multivariante with LASSO','Baseline with Mean']))\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35 # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, mse_reg[:,0,1], width,color='darkorange',label='train',yerr=mse_reg_std[:,0,1], align='center', alpha=0.8, ecolor='black', capsize=1.5,linewidth=0.1)\n",
    "rects2 = ax.bar(x + width/2, mse_reg[:,1,1], width, color='olivedrab', label='test',yerr=mse_reg_std[:,1,1], align='center', alpha=0.5, ecolor='black', capsize=1.5)\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('Drop NaN')\n",
    "ax.set_xticks(x, labels,rotation=90)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_ylim([0,70])\n",
    "fig.savefig('mse_dropna.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['LifeExpectancy','Year'], axis=1)\n",
    "y = data['LifeExpectancy']\n",
    "X = scale(X)\n",
    "\n",
    "pca = PCA()\n",
    "X_reduced = pca.fit_transform(X)\n",
    "cum_exp_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(cum_exp_var)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance ratio')\n",
    "plt.title('Cumulative explained variance ratio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_exp_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training (80%) and test (20%) sets\n",
    "X = data.drop(['LifeExpectancy','Year'], axis=1)\n",
    "y = data['LifeExpectancy']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) \n",
    "\n",
    "# Scale the training set\n",
    "X_train = scale(X_train)\n",
    "# Apply PCA on the training set, set n_component = 2\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced_train = pca.fit_transform(X_train)\n",
    "# Train Linear Regression model on training data \n",
    "reg = LinearRegression()\n",
    "reg.fit(X_reduced_train, y_train)\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)\n",
    "\n",
    "# Scale the test set\n",
    "X_test = scale(X_test)\n",
    "# Apply PCA on test set, set n_component = 2\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced_test = pca.fit_transform(X_test)\n",
    "# Make predictions \n",
    "y_test_pred = reg.predict(X_reduced_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize our regression surface\n",
    "ax = plt.figure(figsize=(10,10)).add_subplot(projection='3d')\n",
    "X = np.arange(min(X_reduced_test[:,0]), max(X_reduced_test[:,0]), 0.001)\n",
    "Y = np.arange(min(X_reduced_test[:,1]), max(X_reduced_test[:,1]), 0.001)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = reg.coef_[0]*X + reg.coef_[1]*Y + reg.intercept_\n",
    "surf = ax.plot_surface(X,Y,Z,color='b',alpha=0.3, label='Regression')\n",
    "surf._edgecolors2d = surf._edgecolor3d\n",
    "surf._facecolors2d = surf._facecolor3d\n",
    "# Visualize our observations\n",
    "ax.scatter(X_reduced_test[:,0], X_reduced_test[:,1], y_test, color='r', label='Observation')\n",
    "# Visualize our predictions\n",
    "ax.scatter(X_reduced_test[:,0], X_reduced_test[:,1], y_test_pred, color='b', label='Prediction')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Principle Component 1')\n",
    "ax.set_ylabel('Principle Component 2')\n",
    "ax.set_zlabel('Life Expectancy')\n",
    "ax.set_title('Life Expectancy (PCA n_component=2)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['LifeExpectancy','Year'], axis=1)\n",
    "y = data['LifeExpectancy']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "cv_scores = []\n",
    "test_scores = []\n",
    "for n in range(1,12):\n",
    "    # Scale the training set\n",
    "    X_train = scale(X_train)\n",
    "    # Apply PCA on the training set, set n_component = n\n",
    "    pca = PCA(n)\n",
    "    X_reduced_train = pca.fit_transform(X_train)\n",
    "    # Train Linear Regression model on training data \n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X_reduced_train, y_train)\n",
    "    # Compute the MSE in cross validation groups\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    cv_scores.append(-cross_val_score(reg, X_reduced_train, y_train, cv=cv, scoring='neg_mean_squared_error').mean())\n",
    "    \n",
    "    # Scale the test set\n",
    "    X_test = scale(X_test)\n",
    "    # Apply PCA on test set, set n_component = n\n",
    "    pca = PCA(n)\n",
    "    X_reduced_test = pca.fit_transform(X_test)\n",
    "    # Make predictions \n",
    "    y_test_pred = reg.predict(X_reduced_test)\n",
    "    # Compute the MSE in test set\n",
    "    test_scores.append(mean_squared_error(y_test, y_test_pred, squared=True))\n",
    "print(cv_scores)\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of component to choose\n",
    "plt.plot(np.arange(1,12),cv_scores,label='Training')\n",
    "plt.plot(np.arange(1,12),test_scores,label='Test')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Mean Squared Error with different number of components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose n_component = 6\n",
    "# Split the dataset into training (80%) and test (20%) sets\n",
    "X = data.drop(['LifeExpectancy','Year'], axis=1)\n",
    "y = data['LifeExpectancy']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) \n",
    "\n",
    "# Scale the training set\n",
    "X_train = scale(X_train)\n",
    "# Apply PCA on the training set, set n_component = 4\n",
    "pca = PCA(n_components=4)\n",
    "X_reduced_train = pca.fit_transform(X_train)\n",
    "# Train Linear Regression model on training data \n",
    "reg = LinearRegression()\n",
    "reg.fit(X_reduced_train, y_train)\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)\n",
    "\n",
    "# Scale the test set\n",
    "X_test = scale(X_test)\n",
    "# Apply PCA on test set, set n_component = 4\n",
    "pca = PCA(n_components=4)\n",
    "X_reduced_test = pca.fit_transform(X_test)\n",
    "# Make predictions \n",
    "y_test_pred = reg.predict(X_reduced_test)\n",
    "\n",
    "plt.scatter(y_test, y_test_pred,marker='.',label='(Observation, Prediction)')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('Prediction')\n",
    "plt.plot(y_test,y_test,c='r',label='y=x')\n",
    "plt.legend()\n",
    "plt.title('Prediction of life expectancy against observation on test set')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "91a3197ca66a8a03d344076738e631644e16a5a5c1a5c784f9331af4e0ecbe63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
